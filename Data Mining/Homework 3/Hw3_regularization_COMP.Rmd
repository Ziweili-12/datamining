---
title: "DNSC 6279 Section 11 Spring 2020 Homework 3"
author: "Ziwei Li"
output:
  html_document: default
  pdf_document: default
subtitle: \textbf{Due on 02/20/2018}
header-includes: \usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amssymb,mathabx,amsthm,bm,bbm}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require(ISLR)) { install.packages("ISLR", repos = "http://cran.us.r-project.org"); library(ISLR) }
if(!require(leaps)) { install.packages("leaps", repos = "http://cran.us.r-project.org"); library(leaps) }
if(!require(glmnet)) { install.packages("glmnet", repos = "http://cran.us.r-project.org"); library(glmnet) }
if(!require(pls)) { install.packages("pls", repos = "http://cran.us.r-project.org"); library(pls) }
```
\theoremstyle{definition}
\newtheorem*{hint}{Hint}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}

*Remark.* This homework aims to help you further understand the model selection techniques in linear model. Credits for **Theoretical Part** and **Computational Part** are in total 100 pt. For **Computational Part** , please complete your answer in the **RMarkdown** file and summit your printed PDF homework created by it.

## Computational Part

**Hint.** Before starting your work, carefully read Textbook Chapter 6.5-6.7 (Lab 1-3). Mimic the related analyses you learn from it. Related packages have been loaded in setup.

1. (Model Selection, Textbook 6.8, *25 pt*) In this exercise, we will generate simulated data, and will then use this data to perform model selection.

    (a) Use the `rnorm` function to generate a predictor $\bm{X}$ of length $n = 100$, as well as a noise vector $\bm{\epsilon}$ of length $n = 100$.
```{r}
set.seed(1)
x = rnorm(100)
epsilon = rnorm(100)
```
    

    (b) Generate a response vector $\bm{Y}$ of length $n = 100$ according to the model $$ Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon, $$ where $\beta_0 = 3$, $\beta_1 = 2$, $\beta_2 = -3$, $\beta_3 = 0.3$.
```{r}
## set the coefficients and formulate the true model
b0 = 3
b1 = 2
b2 = -3
b3 = 0.3
y = b0 + b1 * x + b2 * x^2 + b3 * x^3 + epsilon
```



    (c) Use the `regsubsets` function from `leaps` package to perform best subset selection in order to choose the best model containing the predictors $(X, X^2, \cdots, X^{10})$. What is the best model obtained according to $C_p$, BIC, and adjusted $R^2$? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained.
```{r}
## include variables from x to x^10, and save them into a dataframe
full = data.frame(y=y,x=x,x2=x^2,x3=x^3,x4=x^4,x5=x^5,x6=x^6,x7=x^7,x8=x^8,x9=x^9,x10=x^10)
## perform best subset selection using regsubsets()
regfit1 = regsubsets(y ~ .,data = full,nvmax=10)
regfit1.sum=summary(regfit1)
```


```{r}
par(mfrow=c(2,2))
## plot of cp
plot(regfit1.sum$cp,xlab='Number of Variables',ylab='CP',type="l")
points(which.min(regfit1.sum$cp),regfit1.sum$cp[which.min(regfit1.sum$cp)],col='red',cex=3,pch=16)
## plot of BIC
plot(regfit1.sum$bic,xlab='Number of Variables',ylab='BIC',type="l")
points(which.min(regfit1.sum$bic),regfit1.sum$bic[which.min(regfit1.sum$bic)],col='Blue',cex=3,pch=15)
## plot of Adjusted R-squared
plot(regfit1.sum$adjr2,xlab='Number of Variables',ylab='Adjusted R-Squared',type="l")
points(which.max(regfit1.sum$adjr2),regfit1.sum$adjr2[which.max(regfit1.sum$adjr2)],col='Purple',cex=3,pch=17)

## We may notice that when the number of variables = 3, the CP and BIC is the smallest and Adjusted R-squared is the biggest. Therefore, the best model is when number of variables = 3.
```

```{r}
## Best model's coefficient
coef(regfit1,3)
```

    (d) Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?
```{r}
regfit.back = regsubsets(y~.,data=full,nvmax=10,method='backward')
back1 = summary(regfit.back)
coef(regfit.back,3)
```

```{r}
regfit.for = regsubsets(y~.,data=full,nvmax=10,method='forward')
for1 = summary(regfit.for)
coef(regfit.for,3)
```

```{r}
# plot for backward selection
par(mfrow=c(2,2))
## plot of cp
plot(back1$cp,xlab='Number of Variables',ylab='CP',type="l")
points(which.min(back1$cp),back1$cp[which.min(back1$cp)],col='red',cex=3,pch=16)
## plot of BIC
plot(back1$bic,xlab='Number of Variables',ylab='BIC',type="l")
points(which.min(back1$bic),back1$bic[which.min(back1$bic)],col='Blue',cex=3,pch=15)
## plot of Adjusted R-squared
plot(back1$adjr2,xlab='Number of Variables',ylab='Adjusted R-Squared',type="l")
points(which.max(back1$adjr2),back1$adjr2[which.max(back1$adjr2)],col='Purple',cex=3,pch=17)
```

```{r}
#plot of forward selection
par(mfrow=c(2,2))
## plot of cp
plot(for1$cp,xlab='Number of Variables',ylab='CP',type="l")
points(which.min(for1$cp),for1$cp[which.min(for1$cp)],col='red',cex=3,pch=16)
## plot of BIC
plot(for1$bic,xlab='Number of Variables',ylab='BIC',type="l")
points(which.min(for1$bic),for1$bic[which.min(for1$bic)],col='Blue',cex=3,pch=15)
## plot of Adjusted R-squared
plot(for1$adjr2,xlab='Number of Variables',ylab='Adjusted R-Squared',type="l")
points(which.max(for1$adjr2),for1$adjr2[which.max(for1$adjr2)],col='Purple',cex=3,pch=17)
```

```{r}
# When using the backward forward selection, the number of true variables are all 3, which is equal to the best subset selection. 
# While in best-subset-selection and forward-selection, the best variables are x1, x2, and x7. However, the best variables using backward selection are x1,x2 and x9.

```

    (e) Now fit a LASSO model with `glmnet` function from `glmnet` package to the simulated data, again using $(X,X^2,\cdots,X^{10})$ as predictors. Use cross-validation to select the optimal value of $\lambda$. Create plots of the cross-validation error as a function of $\lambda$. Report the resulting coefficient estimates, and discuss the results obtained.
```{r}
#set x and y
y.l = as.matrix(full[,1])
x.l = as.matrix(full[,2:11])

#use glmnet to fit the lasso regression and use 5-fold cross-validation 
cv.l <- cv.glmnet(x.l,y.l,alpha=1,nfolds=5) 

# Plot the result
plot(cv.l)
```

```{r}
cat("Lambda with smallest CV Error", cv.l$lambda[which.min(cv.l$cvm)],fill=TRUE)

cat("Coefficients", as.numeric(coef(cv.l)),fill=TRUE) 
```
   
```{r}
# After using lasso regression, we can get X, X^2, X^3, and X^5 as variables for this model.
```

    (f) Now generate a response vector $Y$ according to the model $$Y = \beta_0 + \beta_7 X^7 + \epsilon,$$ where $\beta_7 = 7$, and perform best subset selection and the LASSO. Discuss the results obtained.
```{r}
b7 = 7
y2 = b0 + b7 * x^7 + epsilon
full2 = data.frame(y2=y2,x=x,x2=x^2,x3=x^3,x4=x^4,x5=x^5,x6=x^6,x7=x^7,x8=x^8,x9=x^9,x10=x^10)
```

```{r}
#Best-subset-selection
regfit2 = regsubsets(y2 ~ .,data = full2,nvmax=10)
regfit2.sum=summary(regfit2)

# plot the best-subset-selection
par(mfrow=c(2,2))
## plot of cp
plot(regfit2.sum$cp,xlab='Number of Variables',ylab='CP',type="l")
points(which.min(regfit2.sum$cp),regfit2.sum$cp[which.min(regfit2.sum$cp)],col='red',cex=3,pch=16)
## plot of BIC
plot(regfit2.sum$bic,xlab='Number of Variables',ylab='BIC',type="l")
points(which.min(regfit2.sum$bic),regfit2.sum$bic[which.min(regfit2.sum$bic)],col='Blue',cex=3,pch=15)
## plot of Adjusted R-squared
plot(regfit2.sum$adjr2,xlab='Number of Variables',ylab='Adjusted R-Squared',type="l")
points(which.max(regfit2.sum$adjr2),regfit2.sum$adjr2[which.max(regfit2.sum$adjr2)],col='Purple',cex=3,pch=17)
```

```{r}
# When use cp to choose the best model,, the result is when number of variables are 2.

# When use BIC to choose the best model, the result is when number of variables are 1.

# When use Adjusted R_Squared to choose the best model, the result is when number of variables are 4.

```

```{r}
#set x and y
y.l2 = as.matrix(full2[,1])
x.l2 = as.matrix(full2[,2:11])

#use glmnet to fit the lasso regression and use 5-fold cross-validation 
cv.l2 <- cv.glmnet(x.l2,y.l2,alpha=1,nfolds=5) 

# Plot the result
plot(cv.l2)
```

```{r}
min(cv.l2$lambda)
print(cv.l2)
```

```{r}
#When using Lasso, we can get the smallest lambda is 12.37, and the number of non-zero parameters is 1.
```

    
2. (Prediction, Textbook 6.9, *25 pt*) In this exercise, we will predict the number of applications received using the other variables in the `College` data set from `ISLR` package.

    (a) Randomly split the data set into a training set and a test set (1:1).
```{r}
set.seed(1)
#College2 = College
train=sample(nrow(College),size=0.5*nrow(College)) #select training and test data
test=-(train) #select test data
collegetrain = College[train,]
collegetest = College[test,]
```

    (b) Fit a linear model using least squares on the training set, and report the test error obtained.
```{r}
lm1 = lm(Apps~.,data=collegetrain)
value = collegetest[,-2]
pred = predict(lm1,value)
test_error = mean((pred-collegetest$Apps)^2)
test_error
```


    (c) Fit a ridge regression model on the training set, with $\lambda$ chosen by 5-fold cross-validation. Report the test error obtained.
```{r}
# Transfer the dataset into matrix.
x.train=model.matrix(Apps~.,collegetrain)[,-2] #put regressors from training set into a matrix
y.train=collegetrain$Apps #label for training set
x.test=model.matrix(Apps~.,collegetest)[,-2] #put regressors from test set into a matrix
y.test=collegetest$Apps 
```


```{r}
# Perform ridge regression 
ridge.mod = glmnet(x.train,y.train,alpha=0)
cv.ridge = cv.glmnet(x.train,y.train,alpha=0,nfolds = 5)
bestlam_r = cv.ridge$lambda.min

ridge.pred = predict(ridge.mod,s=bestlam_r,newx=x.test)
ridge.err = mean((ridge.pred-y.test)^2)
ridge.err
bestlam_r
```

    (d) Fit a LASSO model on the training set, with $\lambda$ chosen by 5-fold cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates.
```{r}
lasso.mod = glmnet(x.train,y.train,alpha=1)
cv.lasso = cv.glmnet(x.train,y.train,alpha=1,nfolds = 5)
bestlam_l = cv.lasso$lambda.min

lasso.pred = predict(lasso.mod,s=bestlam_l,newx=x.test)
lasso.err = mean((lasso.pred-y.test)^2)
lasso.err
bestlam_l
```

    
    (e) Fit a PCR model on the training set, with $M$ chosen by 5-fold cross-validation. Report the test error obtained, along with the value of $M$ selected by cross-validation.
```{r}
pcr.fit=pcr(Apps~.,data=collegetrain, scale=TRUE, validation="CV",nfold=5)
summary(pcr.fit)
validationplot(pcr.fit,val.type = "MSEP")

pcr.pred=predict(pcr.fit,collegetest,ncomp=17)
pcc.err = mean((collegetest$Apps-pcr.pred)^2)
pcc.err
```

    
    (f) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these four approaches?
```{r}
error = c(test_error,ridge.err,lasso.err,pcc.err)
names(error)=c("Linear","Ridge","Lasso","PCR")
barplot(error)
```

```{r}
# We can see from the plot that all the method's tet error is almost the same. However,the lasso and ridge may perform better than the other methods.
```

```{r}
exp(-6+40*0.05+3.5)/(1+exp(-6+40*0.05+3.5))
```

```{r}
log(x = )
```

